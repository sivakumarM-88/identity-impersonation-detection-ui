{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1b2817",
   "metadata": {},
   "source": [
    "# Audio Classification - Environmental Sounds - CNN-DNN-Librosa\n",
    "\n",
    "We are going to use a subset of the data from ESC-50 dataset from https://dagshub.com/kinkusuma/esc50-dataset. The ESC-50 dataset is a labeled collection of 2000 environmental audio recordings suitable for benchmarking methods of environmental sound classification.\n",
    "We will develop and train a model to classify 8 differnet environment sounds from the above dataset that has 50+ environment sound audio files for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016795d",
   "metadata": {},
   "source": [
    "# Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff52b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a619fe",
   "metadata": {},
   "source": [
    "# Defining labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d0fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_path=r'C:\\\\Users\\\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\kaggle\\\\archive\\\\KAGGLE\\AUDIO'\n",
    "inference_categories=os.listdir(audio_data_path)\n",
    "category_count=len(inference_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9869942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE', 'REAL']\n"
     ]
    }
   ],
   "source": [
    "print (inference_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3ba53",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ced76f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir, classes, target_shape=(200, 200)):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "                audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "                # Perform preprocessing (e.g., convert to Mel spectrogram and resize)\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "                mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "                data.append(mel_spectrogram)\n",
    "                labels.append(i)\n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d11c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "data, labels = load_and_preprocess_data(audio_data_path, inference_categories)\n",
    "labels = to_categorical(labels, num_classes=len(inference_categories))  # Convert labels to one-hot encoding\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6465f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ab785",
   "metadata": {},
   "source": [
    "# Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d628290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training when the loss falls below 0.1\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "    # Check the loss\n",
    "    if(logs.get('loss') < 0.1):\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.1 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "                \n",
    "    \n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86d7a8",
   "metadata": {},
   "source": [
    "# Creating and compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac75d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model=Sequential([\n",
    "        Conv2D(64,(3,3),activation='relu',input_shape=X_train[0].shape),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(128,(3,3),activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Conv2D(256,(3,3),activation='relu'),\n",
    "        MaxPooling2D(2,2),\n",
    "        Flatten(),\n",
    "        Dense(512,activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(256,activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(category_count,activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bb242",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99727165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yoffi Nelluri\\Raju AI Kadi\\voice\\GOF_hackathon_ImpersV3\\hack7\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - accuracy: 0.5487 - loss: 6.1867 - val_accuracy: 0.7692 - val_loss: 24.4710\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.8960 - loss: 10.7338 - val_accuracy: 0.2308 - val_loss: 6.8463\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.6187 - loss: 6.6832 - val_accuracy: 0.7692 - val_loss: 1.6546\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8860 - loss: 1.6120 - val_accuracy: 0.7692 - val_loss: 0.9696\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.8960 - loss: 0.4973 - val_accuracy: 0.7692 - val_loss: 0.5853\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9158 - loss: 0.2689 - val_accuracy: 0.7692 - val_loss: 0.4938\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9556 - loss: 0.1182 - val_accuracy: 0.7692 - val_loss: 0.5078\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9654 - loss: 0.2363 - val_accuracy: 0.7692 - val_loss: 0.6026\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9603 - loss: 0.0996\n",
      "Loss is lower than 0.1 so cancelling training!\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9604 - loss: 0.0932 - val_accuracy: 0.7692 - val_loss: 0.6325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e34aaa52d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=create_model()\n",
    "model.fit(X_train,y_train,epochs=100,verbose=1,batch_size=25,validation_data=(X_test,y_test),callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17c5b6f",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5df0e10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7692307829856873\n"
     ]
    }
   ],
   "source": [
    "#Validate the model on test dataset (X_test,y_test) \n",
    "test_accuracy=model.evaluate(X_test,y_test,verbose=0)\n",
    "print(test_accuracy[1])\n",
    "\n",
    "# Save the model\n",
    "model.save('FinalModel.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218460",
   "metadata": {},
   "source": [
    "# Evaluate with a random .wav file data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ad22f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583ms/step\n",
      "The audio is classified as: FAKE\n",
      "Accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "#model = load_model('audio_classification_model.h5')\n",
    "##model = load_model('audio_classification_model.h5')\n",
    "# Define the target shape for input spectrograms\n",
    "target_shape = (200, 200)\n",
    "inference_categories = ['FAKE','REAL']\n",
    "                      \n",
    "# Function to preprocess and classify an audio file\n",
    "def test_audio(file_path, model):\n",
    "    # Load and preprocess the audio file\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "    mel_spectrogram = resize(np.expand_dims(mel_spectrogram, axis=-1), target_shape)\n",
    "    mel_spectrogram = tf.reshape(mel_spectrogram, (1,) + target_shape + (1,))\n",
    "\n",
    "    # Make predictions\n",
    "    model2 = load_model(\"FinalModel.h5\")\n",
    "    predictions = model2.predict(mel_spectrogram)\n",
    "\n",
    "    # Get the class probabilities\n",
    "    class_probabilities = predictions[0]\n",
    "\n",
    "    # Get the predicted class index\n",
    "    predicted_class_index = np.argmax(class_probabilities)\n",
    "\n",
    "    return class_probabilities, predicted_class_index\n",
    "\n",
    "# Test an audio file - FAKE\n",
    "#test_audio_file = 'C:\\\\Users\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\challenge-assets-main\\\\challenge-assets-main\\\\identity-personification\\\\synthetic-audio-train\\\\audio_file (2).wav'\n",
    "#test_audio_file = 'C:\\\\Users\\\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\challenge-assets-main\\\\challenge-assets-main\\\\identity-personification\\\\synthetic-audio-train\\\\1.3_Audio_HM.wav'\n",
    "#test_audio_file = 'C:\\\\Users\\\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\kaggle\\\\archive\\\\DEMONSTRATION\\\\DEMONSTRATION\\\\linus-original-DEMO.mp3'\n",
    "test_audio_file = 'C:\\\\Users\\\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\kaggle\\\\archive\\\\DEMONSTRATION\\\\DEMONSTRATION\\\\linus-to-musk-DEMO.mp3'\n",
    "# Test an audio file - FAKE\n",
    "#test_audio_file = 'C:\\\\Users\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\challenge-assets-main\\\\challenge-assets-main\\\\identity-personification\\\\synthetic-audio-train\\\\audio_file (2).wav'\n",
    "#test_audio_file = 'C:\\\\Users\\Yoffi Nelluri\\\\Raju AI Kadi\\\\voice\\\\challenge-assets-main\\\\challenge-assets-main\\\\identity-personification\\\\human_voice_train_data2\\\\human_voice_Script3.mp3'\n",
    "class_probabilities, predicted_class_index = test_audio(test_audio_file, model)\n",
    "\n",
    "# Display results for all classes\n",
    "for i, class_label in enumerate(inference_categories):\n",
    "    probability = class_probabilities[i]\n",
    "    #print(f'Class: {class_label}, Probability: {probability:.4f}')\n",
    "\n",
    "# Calculate and display the predicted class and accuracy\n",
    "predicted_class = inference_categories[predicted_class_index]\n",
    "accuracy = class_probabilities[predicted_class_index]\n",
    "if accuracy<0.75:\n",
    "    predicted_class ='REAL'\n",
    "print(f'The audio is classified as: {predicted_class}')\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89eeca79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'datetime' has no attribute 'now'"
     ]
    }
   ],
   "source": [
    "class_probabilities = predictions_out[0] \n",
    "predicted_class_index = np.argmax(class_probabilities)\n",
    "print (predicted_class_index)\n",
    "#result = 90                    \n",
    "st.info(f\"Result probability: {predicted_class_index * 100:.2f}\")\n",
    "st.success(f\"The uploaded audio is {predicted_class_index * 100:.2f}% likely to be {class_probabilities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
